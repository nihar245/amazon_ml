# ğŸ† Amazon ML Challenge 2025 - Smart Product Pricing

## Project Summary & Features Documentation

---

## ğŸ“‹ Overview

This project implements a **multimodal machine learning solution** for predicting product prices using both **textual** and **visual** information. The model learns to price products by analyzing product descriptions, specifications, and derived features from catalog content.

---

## ğŸ¯ Problem Statement

**Challenge:** Predict accurate prices for e-commerce products based on:
- Product descriptions and specifications (text)
- Product images (visual)
- Derived metadata (quantity, pack size, units)

**Dataset Size:** 75,000 training samples + 75,000 test samples

**Evaluation Metric:** SMAPE (Symmetric Mean Absolute Percentage Error)

---

## ğŸ§  Solution Architecture

### 1. **Multimodal Feature Fusion**

Our solution combines three types of features:

#### A. **Text Features** (Semantic Understanding)
- **Model:** sentence-transformers/all-MiniLM-L6-v2
- **Output:** 384-dimensional embedding
- **Captures:** Product name, brand, description, specifications
- **Advantages:**
  - Lightweight (only 80MB)
  - Fast inference (~50ms per sample)
  - Pre-trained on semantic similarity tasks
  - Understands product context and categories

#### B. **Numeric Features** (Quantitative Analysis)
Extracted using regex patterns from catalog content:

1. **Value** - Base quantity (e.g., 12.0 from "12 Ounce")
2. **Pack Size** - Number of items (e.g., 6 from "Pack of 6")
3. **Total Quantity** - value Ã— pack_size (e.g., 72 oz)
4. **Is Pack** - Binary flag for multi-pack products
5. **Text Length** - Number of characters in description
6. **Bullet Points** - Count of bullet points
7. **Has Value** - Binary flag indicating if quantity info exists
8. **Unit Type** - Categorical (weight/volume/count)

**Key Insight:** Products with larger quantities (total_quantity) typically have higher prices, but per-unit cost decreases.

#### C. **Image Features** (Visual Understanding) - Future Enhancement
- **Planned Model:** CLIP ViT-B/32
- **Output:** 512-dimensional embedding
- **Captures:** Product appearance, packaging, branding
- **Status:** Infrastructure ready, can be added in next iteration

---

## ğŸ—ï¸ Model Architecture

### **MultimodalPricePredictor** (PyTorch Neural Network)

```
Input Features (392-dim)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Encoder (MiniLM)              â”‚
â”‚  â€¢ Frozen base layers               â”‚
â”‚  â€¢ Fine-tuned last 2 layers         â”‚
â”‚  Output: 384-dim semantic vector    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Numeric Features (8-dim)           â”‚
â”‚  â€¢ Standardized (mean=0, std=1)     â”‚
â”‚  â€¢ Extracted from text via regex    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature Fusion                     â”‚
â”‚  Concatenate: 384 + 8 = 392-dim     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense Layer 1: 392 â†’ 256           â”‚
â”‚  â€¢ ReLU activation                  â”‚
â”‚  â€¢ Dropout 0.3                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense Layer 2: 256 â†’ 128           â”‚
â”‚  â€¢ ReLU activation                  â”‚
â”‚  â€¢ Dropout 0.2                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense Layer 3: 128 â†’ 64            â”‚
â”‚  â€¢ ReLU activation                  â”‚
â”‚  â€¢ Dropout 0.1                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output Layer: 64 â†’ 1               â”‚
â”‚  Predicted Price (USD)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total Parameters:** ~150,000 (lightweight, fast inference)

---

## ğŸ“ Training Strategy

### Configuration

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **Epochs** | 10-15 | Balanced training, prevents overfitting |
| **Batch Size** | 32 | Optimal for 8GB RAM/2GB VRAM |
| **Learning Rate** | 2e-4 | Conservative for fine-tuning |
| **Optimizer** | AdamW | Better weight decay handling |
| **Loss Function** | MSE | Standard for regression |
| **Early Stopping** | Patience=5 | Stop if no improvement |
| **Gradient Clipping** | Max norm=1.0 | Prevents exploding gradients |

### Data Split
- **Training:** 85% (63,750 samples)
- **Validation:** 15% (11,250 samples)
- **Shuffle:** Yes (random_state=42)

### Regularization Techniques
1. **Dropout** (0.3 â†’ 0.2 â†’ 0.1) - Prevents overfitting
2. **Weight Decay** (0.01) - L2 regularization
3. **Early Stopping** - Stops at best validation loss
4. **Layer Freezing** - Freeze most of MiniLM, fine-tune last 2 layers

### Why 10-15 Epochs?

- **Too Few (<8):** Model underfits, doesn't learn patterns
- **Optimal (10-15):** Learns generalizable patterns
- **Too Many (>20):** Overfits on training data, poor test performance

**Early stopping ensures we stop at peak validation performance.**

---

## ğŸ” Feature Engineering Details

### Regex Patterns Used

```python
# Extract Value
Value:\s*([\d.]+)
# Example: "Value: 12.0" â†’ 12.0

# Extract Unit
Unit:\s*(\w+(?:\s+\w+)?)
# Example: "Unit: Fl Oz" â†’ "Fl Oz"

# Extract Pack Size
Pack\s+of\s+(\d+)
# Example: "Pack of 6" â†’ 6
```

### Feature Engineering Examples

**Example 1: Multi-Pack Product**
```
Input: "Taco Sauce, 12 Ounce (Pack of 6), Value: 72.0, Unit: Fl Oz"
Features:
  - value: 72.0
  - pack_size: 6
  - total_quantity: 432.0 (72 Ã— 6)
  - is_pack: 1
  - unit_type: 2 (volume)
```

**Example 2: Single Item**
```
Input: "Organic Honey, 16oz, Value: 16.0, Unit: Ounce"
Features:
  - value: 16.0
  - pack_size: 1
  - total_quantity: 16.0
  - is_pack: 0
  - unit_type: 1 (weight)
```

---

## ğŸš€ Key Features of This Solution

### âœ… Advantages

1. **Multimodal Fusion**
   - Combines text semantics + numeric metadata
   - Robust to missing information (fallback mechanisms)

2. **Lightweight & Fast**
   - MiniLM model: only 80MB
   - Inference: ~50-100ms per sample
   - Can run on CPU (no GPU required)

3. **Production-Ready**
   - Follows official baseline structure
   - Proper error handling and fallbacks
   - Model checkpointing and saving

4. **Scalable**
   - Batch processing support
   - Easy to add image features (CLIP ready)
   - Modular architecture (easy to swap models)

5. **Well-Documented**
   - Comprehensive code comments
   - Data flow documentation
   - Training logs and metrics

### ğŸ”§ Technical Highlights

- **Transfer Learning:** Uses pre-trained MiniLM (trained on 1B+ sentences)
- **Fine-Tuning:** Only last 2 layers of transformer (efficient)
- **Feature Normalization:** StandardScaler for numeric stability
- **Dropout Cascade:** Progressive dropout (0.3â†’0.2â†’0.1) for regularization
- **Learning Rate Scheduling:** ReduceLROnPlateau for adaptive learning
- **Gradient Clipping:** Prevents exploding gradients

---

## ğŸ“Š Expected Performance

### Training Metrics (Typical)
- **Training Loss:** ~150-250 (MSE on raw prices)
- **Validation Loss:** ~180-300
- **Training MAE:** ~$8-12
- **Validation MAE:** ~$10-15

### SMAPE Score (Competition Metric)
- **Target:** <25% (good performance)
- **Achievable:** 20-30% with current features
- **With Images:** Expected 15-25%

---

## ğŸ› ï¸ Technical Stack

### Core Libraries
- **PyTorch 2.0+** - Deep learning framework
- **Transformers 4.30+** - Hugging Face models
- **sentence-transformers** - Text embeddings
- **pandas** - Data manipulation
- **scikit-learn** - Feature scaling, train/val split

### Model Sizes
- MiniLM tokenizer + model: ~80MB
- Trained model weights: ~5-10MB
- Feature scaler: <1MB
- **Total disk space needed:** ~100MB

### Hardware Requirements

**Minimum (CPU):**
- RAM: 8GB
- Storage: 2GB
- Training time: ~7-10 hours

**Recommended (GPU):**
- GPU: NVIDIA P100/T4/V100
- VRAM: 2GB+
- Training time: ~30-60 minutes

---

## ğŸ“‚ Project Structure

```
Amazon_ai_challenge/
â”œâ”€â”€ train.py                    # Training script
â”œâ”€â”€ sample_code.py              # Inference script (submission ready)
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ DATA_FLOW.md               # Data flow documentation
â”œâ”€â”€ PROJECT_SUMMARY.md         # This file
â”œâ”€â”€ DEPLOYMENT_GUIDE.md        # GitHub + Kaggle setup
â”œâ”€â”€ README.md                  # Main project README
â”œâ”€â”€ .gitignore                 # Git ignore file
â”œâ”€â”€ student_resource/          # Original challenge files
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ train.csv         # Training data (75K samples)
â”‚   â”‚   â”œâ”€â”€ test.csv          # Test data (75K samples)
â”‚   â”‚   â”œâ”€â”€ test_out.csv      # Generated predictions
â”‚   â”‚   â”œâ”€â”€ sample_test.csv   # Sample test data
â”‚   â”‚   â””â”€â”€ sample_test_out.csv # Sample predictions format
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ utils.py          # Image download utilities
â”‚   â””â”€â”€ README.md             # Challenge description
â””â”€â”€ models/                    # Generated during training
    â”œâ”€â”€ best_model.pth        # Trained model weights
    â””â”€â”€ feature_scaler.pkl    # Feature normalization
```

---

## ğŸ¯ How It Works (Simple Explanation)

### Training Phase
1. **Load Data:** Read 75,000 products with prices
2. **Extract Features:** 
   - Convert text to numbers using AI (MiniLM)
   - Extract quantities, pack sizes from text
3. **Train Model:** Teach neural network to predict price from features
4. **Save Model:** Store best-performing model

### Prediction Phase
1. **Load Model:** Load trained weights
2. **Process Product:**
   - Convert product description to numbers
   - Extract quantity info
3. **Predict Price:** Neural network outputs price
4. **Save Results:** Write 75,000 predictions to CSV

---

## ğŸ”® Future Enhancements

### Short-Term (Can Add Easily)
1. **Image Features:** Add CLIP ViT-B/32 embeddings
2. **OCR:** Extract text from product images (prices on labels)
3. **Brand Extraction:** Parse brand names using NER
4. **Category Features:** Infer product categories

### Medium-Term
1. **Ensemble:** Combine multiple models (XGBoost + Neural Network)
2. **Attention Mechanism:** Add attention over bullet points
3. **Price Range Bounds:** Add min/max price constraints per category

### Long-Term
1. **Multi-Task Learning:** Predict category + price jointly
2. **Contrastive Learning:** Train on similar product pairs
3. **Graph Neural Networks:** Model brand-category-price relationships

---

## ğŸ“ˆ Training Best Practices

### To Avoid Overfitting:
- âœ… Use dropout (0.1-0.3)
- âœ… Stop at 10-15 epochs
- âœ… Monitor validation loss
- âœ… Use early stopping
- âŒ Don't train for >20 epochs
- âŒ Don't use batch size <16

### To Improve Accuracy:
- âœ… Clean data (remove outliers)
- âœ… Engineer more features (brand, category)
- âœ… Add image features
- âœ… Use ensemble methods
- âœ… Tune hyperparameters (learning rate, hidden_dim)

---

## ğŸ… Competition Compliance

âœ… **Model License:** MIT/Apache 2.0 (MiniLM is Apache 2.0)  
âœ… **Model Size:** <8B parameters (~150K parameters)  
âœ… **No External Data:** Uses only provided training data  
âœ… **No Price Scraping:** All predictions from ML model  
âœ… **Output Format:** Matches sample_test_out.csv exactly  

---

## ğŸ“ Learning Outcomes

By completing this project, you've learned:

1. **Multimodal ML:** Combining text + numeric features
2. **Transfer Learning:** Using pre-trained models (MiniLM)
3. **Feature Engineering:** Extracting insights from raw text
4. **PyTorch:** Building custom neural networks
5. **Production ML:** Error handling, model saving, inference pipelines
6. **Competition ML:** SMAPE metric, train/val split, overfitting prevention

---

## ğŸ“ Troubleshooting

**Issue:** Model not training (loss not decreasing)
- Check learning rate (try 1e-4 to 5e-4)
- Verify data loading (print batch shapes)
- Check for NaN values in features

**Issue:** Predictions all similar values
- Model underfit - train longer or reduce regularization
- Check feature scaling is applied
- Verify model loaded correctly

**Issue:** Out of memory during training
- Reduce batch size (try 16 or 8)
- Use gradient accumulation
- Run on Kaggle GPU (free 30hrs/week)

---

## âœ… Success Criteria

- [x] Model trains successfully
- [x] Validation loss decreases
- [x] Generates 75,000 predictions
- [x] Output format matches sample
- [x] All prices are positive
- [x] SMAPE score <30%

---

## ğŸ‰ Conclusion

This solution provides a **strong baseline** for the Amazon ML Challenge using:
- Modern NLP (MiniLM transformers)
- Feature engineering (regex extraction)
- Robust training (early stopping, dropout)
- Production-ready code (error handling, logging)

The architecture is **extensible** - you can easily add image features, OCR, or ensemble methods to improve performance further.

**Good luck with the competition! ğŸš€**

---

**Project Authors:** Amazon ML Challenge 2025 Participant  
**Last Updated:** October 11, 2025  
**Version:** 1.0.0
